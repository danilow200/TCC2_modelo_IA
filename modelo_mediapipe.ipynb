{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Pasta com os vídeos\n",
    "pasta_videos = './completo - Copia'\n",
    "\n",
    "# Listas para armazenar os dados e os rótulos\n",
    "dados_direita = []\n",
    "dados_esquerda = []\n",
    "rotulos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danil\\Documents\\GitHub\\TCC2_modelo_IA\\myenv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\danil\\Documents\\GitHub\\TCC2_modelo_IA\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média do modelo com validação cruzada: 95.86%\n",
      "Acurácia do modelo no conjunto de teste: 95.65%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler_esquerda.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dicionário para armazenar as coordenadas por palavra\n",
    "coordenadas_por_palavra = {'direita': {}, 'esquerda': {}}\n",
    "\n",
    "for subpasta in os.listdir(pasta_videos):\n",
    "    for video in os.listdir(os.path.join(pasta_videos, subpasta)):\n",
    "        # Lendo o vídeo\n",
    "        cap = cv2.VideoCapture(os.path.join(pasta_videos, subpasta, video))\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convertendo a cor da imagem\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Processando a imagem\n",
    "            result = hands.process(image)\n",
    "            \n",
    "            if result.multi_hand_landmarks and result.multi_handedness:\n",
    "                coords_direita = None\n",
    "                coords_esquerda = None\n",
    "                \n",
    "                for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "                    # Identificar se a mão é direita ou esquerda\n",
    "                    hand_label = handedness.classification[0].label.lower()  # 'left' ou 'right'\n",
    "                    if hand_label == 'right':\n",
    "                        hand_label = 'direita'\n",
    "                        coords_direita = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmarks.landmark]).flatten())\n",
    "                    else:\n",
    "                        hand_label = 'esquerda'\n",
    "                        coords_esquerda = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmarks.landmark]).flatten())\n",
    "                    \n",
    "                # Adicionando as coordenadas à lista de dados\n",
    "                if coords_direita:\n",
    "                    dados_direita.append(coords_direita)\n",
    "                if coords_esquerda:\n",
    "                    dados_esquerda.append(coords_esquerda)\n",
    "                    \n",
    "                # Adicionando o rótulo à lista de rótulos\n",
    "                rotulos.append(subpasta)\n",
    "                \n",
    "                # Armazenando as coordenadas por palavra e mão\n",
    "                if subpasta not in coordenadas_por_palavra['direita']:\n",
    "                    coordenadas_por_palavra['direita'][subpasta] = []\n",
    "                if subpasta not in coordenadas_por_palavra['esquerda']:\n",
    "                    coordenadas_por_palavra['esquerda'][subpasta] = []\n",
    "                if coords_direita:\n",
    "                    coordenadas_por_palavra['direita'][subpasta].append(tuple(coords_direita))\n",
    "                if coords_esquerda:\n",
    "                    coordenadas_por_palavra['esquerda'][subpasta].append(tuple(coords_esquerda))\n",
    "\n",
    "# Selecionando os 30% movimentos mais frequentes por palavra e por mão\n",
    "coordenadas_selecionadas = {'direita': {}, 'esquerda': {}}\n",
    "for hand_label in ['direita', 'esquerda']:\n",
    "    for palavra, coords_lista in coordenadas_por_palavra[hand_label].items():\n",
    "        # Contar a frequência de cada conjunto de coordenadas\n",
    "        contador = Counter(coords_lista)\n",
    "        # Ordenar os conjuntos de coordenadas por frequência\n",
    "        mais_frequentes = sorted(contador.items(), key=lambda item: item[1], reverse=True)\n",
    "        # Selecionar os 30% conjuntos de coordenadas mais frequentes\n",
    "        top_30_percent = mais_frequentes[:max(1, len(mais_frequentes) * 30 // 100)]\n",
    "        coordenadas_selecionadas[hand_label][palavra] = [item[0] for item in top_30_percent]\n",
    "\n",
    "# Preparando os dados para treinamento\n",
    "dados_filtrados_direita = []\n",
    "dados_filtrados_esquerda = []\n",
    "rotulos_filtrados = []\n",
    "for hand_label in ['direita', 'esquerda']:\n",
    "    for palavra, coords_lista in coordenadas_selecionadas[hand_label].items():\n",
    "        for coords in coords_lista:\n",
    "            if hand_label == 'direita':\n",
    "                dados_filtrados_direita.append(coords)\n",
    "            else:\n",
    "                dados_filtrados_esquerda.append(coords)\n",
    "            rotulos_filtrados.append(palavra)\n",
    "\n",
    "# Convertendo as listas em arrays numpy\n",
    "dados_direita = np.array(dados_filtrados_direita)\n",
    "dados_esquerda = np.array(dados_filtrados_esquerda)\n",
    "rotulos = np.array(rotulos_filtrados)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler_direita = StandardScaler()\n",
    "scaler_esquerda = StandardScaler()\n",
    "dados_direita = scaler_direita.fit_transform(dados_direita)\n",
    "dados_esquerda = scaler_esquerda.fit_transform(dados_esquerda)\n",
    "\n",
    "# Balanceamento dos dados\n",
    "dados_balanceados_direita = []\n",
    "dados_balanceados_esquerda = []\n",
    "rotulos_balanceados = []\n",
    "\n",
    "for palavra in set(rotulos):\n",
    "    indices_direita = [i for i, r in enumerate(rotulos) if r == palavra and i < len(dados_direita)]\n",
    "    indices_esquerda = [i for i, r in enumerate(rotulos) if r == palavra and i < len(dados_esquerda)]\n",
    "    min_amostras = min(len(indices_direita), len(indices_esquerda))\n",
    "    if min_amostras > 0:\n",
    "        indices_selecionados_direita = np.random.choice(indices_direita, min_amostras, replace=False)\n",
    "        indices_selecionados_esquerda = np.random.choice(indices_esquerda, min_amostras, replace=False)\n",
    "        dados_balanceados_direita.extend(dados_direita[indices_selecionados_direita])\n",
    "        dados_balanceados_esquerda.extend(dados_esquerda[indices_selecionados_esquerda])\n",
    "        rotulos_balanceados.extend(rotulos[indices_selecionados_direita])\n",
    "\n",
    "dados_balanceados_direita = np.array(dados_balanceados_direita)\n",
    "dados_balanceados_esquerda = np.array(dados_balanceados_esquerda)\n",
    "rotulos_balanceados = np.array(rotulos_balanceados)\n",
    "\n",
    "# Concatenando as coordenadas das mãos direita e esquerda\n",
    "dados_balanceados = np.concatenate((dados_balanceados_direita, dados_balanceados_esquerda), axis=1)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(dados_balanceados, rotulos_balanceados, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinando o modelo com as coordenadas selecionadas\n",
    "modelo = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42, class_weight='balanced')\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Avaliando o modelo com validação cruzada\n",
    "scores = cross_val_score(modelo, dados_balanceados, rotulos_balanceados, cv=5)\n",
    "print(f'Acurácia média do modelo com validação cruzada: {scores.mean()*100:.2f}%')\n",
    "\n",
    "# Avaliando o modelo\n",
    "score = modelo.score(X_test, y_test)\n",
    "print(f'Acurácia do modelo no conjunto de teste: {score*100:.2f}%')\n",
    "\n",
    "# Salvando o modelo e os scalers\n",
    "joblib.dump(modelo, 'modelo.pkl')\n",
    "joblib.dump(scaler_direita, 'scaler_direita.pkl')\n",
    "joblib.dump(scaler_esquerda, 'scaler_esquerda.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
